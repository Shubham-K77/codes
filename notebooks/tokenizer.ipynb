{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db94890e-cf7b-4ccb-9a96-fcfa90f41737",
   "metadata": {},
   "source": [
    "# Reading a short story as text sample in python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a571b4-d04a-4c40-aba7-c1021006bbd5",
   "metadata": {},
   "source": [
    "## Step 1: Creating tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227bc34d-597c-4b9f-b9a5-5e535298fddd",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: green; padding: 10px; border-radius: 5px;\">\n",
    "  The print command prints the total number of characters followed by the first 100 characters of this file for illustrations.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf8c8a6b-0fc1-4e1f-8678-7f98a1a05990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters:  20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"datasets/theVerdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Total number of characters: \", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7104fd4f-45ed-4e6a-803f-d592f800bbf7",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: green; padding: 10px; border-radius: 5px;\">\n",
    "    Our goal is to tokenize the 20479 characters short story into individual words and special characters that we can then turn into vector embeddings for LLM training.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71805090-83cb-4110-afdd-0d6564a9628c",
   "metadata": {},
   "source": [
    "**Regular Expression Library is used to split the sentences into individual texts.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c374715-6c06-4d89-ae33-7f8edcb49b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello, world. This, is a test\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60d59f1-db9e-4a10-b71d-1c9127bcc9f4",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: green; padding: 10px; border-radius: 5px;\">\n",
    "    The /s is regular expression for the whitespace so this will split the sentence according to white-spaces found.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491703d0-9ff6-4200-908f-6916217e2cce",
   "metadata": {},
   "source": [
    "**We can modify to split the whitespaces, periods and commas also i.e [,.]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87c40d06-41bd-4e3c-9a9e-f33133d97811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc62f105-da75-4aff-8db2-937614e5e4d7",
   "metadata": {},
   "source": [
    "**Removing The WhiteSpaces Safely**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb2478ec-36c4-4ffb-8ac1-7e624670b0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e28760e-cc4c-4e86-a991-d2d31f9f5644",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: green; padding: 10px; border-radius: 5px;\">\n",
    "    Firstly, for item in result is used to iterate through every items in the list, this will only return true if there is no white-spaces found, hence it will return the item and be stored in a newList allocated to result.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813d55b8-0854-4aeb-8565-a22c9b4f200c",
   "metadata": {},
   "source": [
    "**Modifying The Tokenizer Further To Accept quotations, question marks, double-dashes as a separate special characters or tokens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "154c600c-5a4e-4d70-8df7-917ca9d7ec37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18654fae-fd3c-4023-8fbf-765466d7ead5",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: green; padding: 10px; border-radius: 5px;\">\n",
    "    We use double item.strip() cause when splitting the special characters it might contain some whitespaces again so double checking ensures that no whitespacing is included in the final text.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f589f3b-adc6-4654-8d03-9b35d047a9f0",
   "metadata": {},
   "source": [
    "**Applying the tokenizer scheme to entire raw text data of the book**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "035d70c9-310d-4f59-a113-073f51757ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preProcessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preProcessed = [item.strip() for item in preProcessed if item.strip()]\n",
    "print(preProcessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44a95fce-78df-46eb-b9e4-dec6d0202749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "print(len(preProcessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304f175a-1489-4aca-ad95-4e29b0a829be",
   "metadata": {},
   "source": [
    "# Step 2: Assigning Token Id's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0fd221-9984-48d4-892f-3f9833565f4b",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: green; padding: 10px; border-radius: 5px;\">\n",
    "    Now, we take all the unique tokens together and sort them alphabetically\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7063407a-031a-4000-a27f-6077d3840584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preProcessed))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8125c1-8233-4d5f-b8b8-016fc1922a84",
   "metadata": {},
   "source": [
    "**Creating our own vocabulary with all the unique words by assigning integer values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bca50409-5f45-4715-98ce-554530b85012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token: integer for integer, token in enumerate(all_words)}\n",
    "for i,item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3d1198-91d9-4ad2-b56c-3dc9764e85a9",
   "metadata": {},
   "source": [
    "**Now Creating a class in python for encoding and decoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ae33465a-3222-4127-9b1e-85289758733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleTokenizerV1:\n",
    "    def __init__ (self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode (self, text):\n",
    "        preprocessed = re.split(r'([.,:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decoder (self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Remove the whitespaces before any specific characters:\n",
    "        text = re.sub(r'\\s+([,.:_?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a083c4-fbfe-40d1-be51-9352bdb02669",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: green; padding: 10px; border-radius: 5px;\">\n",
    "    Now, we instantiate the tokenizer class and try to pass the data. Encode Method converts the string into integers or token id's according to our vocabulary similarly, decode takes the list of ids and converts it back to the string.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b43ae742-1e29-4837-bc20-a80a8a2df4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 63, 2, 850, 731, 406, 584, 1016, 806, 203, 579, 265, 5, 1, 1077, 729, 722, 988, 189, 533, 598, 362, 127, 988, 87, 157, 890, 722, 156, 411, 168, 651, 5, 1090, 5, 727, 115, 604, 315, 5, 53, 514, 140, 849, 741, 477, 64, 24, 9, 157, 67, 7, 38, 5, 199, 727, 546, 5, 130, 456, 697, 391, 8, 1, 57, 584, 908, 684, 868, 1016, 403, 464, 722, 203, 7, 1]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = simpleTokenizerV1(vocab)\n",
    "text = \"\\\"Money's only excuse is to put beauty into circulation,\\\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed luncheon-table, when, on a later day, I had again run over from Monte Carlo; and Mrs. Gisburn, beaming on him, added for my enlightenment: \\\"Jack is so morbidly sensitive to every form of beauty.\\\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32a6b2e4-a9f1-479a-ba25-a0a9b72c9d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" Money' s only excuse is to put beauty into circulation,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed luncheon-table, when, on a later day, I had again run over from Monte Carlo ; and Mrs. Gisburn, beaming on him, added for my enlightenment:\" Jack is so morbidly sensitive to every form of beauty.\"\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decoder(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a609e7f1-03e9-4e7a-b43c-3889dedf8810",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow; color: orange; padding: 10px; border-radius: 5px;\">\n",
    "   What if we send a sentence that doesn't belong to the training set or vocabulary?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26ee43c4-e162-484e-b051-280ca0d67f57",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, do you like some tea?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(ids)\n",
      "Cell \u001b[1;32mIn[22], line 9\u001b[0m, in \u001b[0;36msimpleTokenizerV1.encode\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m      7\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([.,:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m      8\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[1;32m----> 9\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "Cell \u001b[1;32mIn[22], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      7\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([.,:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m      8\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[1;32m----> 9\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Hello'"
     ]
    }
   ],
   "source": [
    "text = \"Hello, do you like some tea?\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24069678-e67e-4383-8090-94aed2df38b2",
   "metadata": {},
   "source": [
    "## Adding Special Context Tokens To Deal with data that is beyond the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9934ee-bef5-4090-b4e4-2747a178abd8",
   "metadata": {},
   "source": [
    "**We add two new tokens: <|unk|> which is used for token not part of the vocab and <|endoftext|> which was used for GPT pre-training to specify that the new text source is being used.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad0d7aef-ec03-499f-b33a-3ed2a2528706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preProcessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token: id for id, token in enumerate(all_tokens)}\n",
    "print(len(vocab.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4428c6c6-566b-4545-9521-ceefb0887543",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: green; padding: 10px; border-radius: 5px;\">\n",
    "    This increased the size of the vocabulary to 1132 previously it was 1130\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48278ba8-8155-4060-8207-7e7732372881",
   "metadata": {},
   "source": [
    "**Quickly printing the last 5 words in the vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42a9f631-2424-4d64-beda-d34756c6b98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5224954e-28ee-43d8-a2d8-dd545bf99da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleTokenizerV2:\n",
    "    def __init__ (self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode (self, text):\n",
    "        preprocessed = re.split(r'([.,:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_int\n",
    "                       else \"<|unk|>\" for item in preprocessed\n",
    "                       ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode (self, ids):\n",
    "        text = (\" \").join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.:_?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6673b080-1a23-4fa3-8ce3-9c1b51fc560b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 5, 355, 1126, 628, 910, 975, 10]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = simpleTokenizerV2(vocab)\n",
    "text = \"Hello, do you like some tea?\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8006dc28-9f63-4a10-a5bf-b026e9399c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like some tea?<|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like some tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \"<|endoftext|> \".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6a6cf0f3-f4d2-479a-8b63-0464d87a20a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131,\n",
       " 5,\n",
       " 355,\n",
       " 1126,\n",
       " 628,\n",
       " 910,\n",
       " 975,\n",
       " 10,\n",
       " 1130,\n",
       " 55,\n",
       " 988,\n",
       " 956,\n",
       " 984,\n",
       " 722,\n",
       " 988,\n",
       " 1131,\n",
       " 7]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618c333b-126e-40b0-bf07-6655920d72ff",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: green; padding: 10px; border-radius: 5px;\">\n",
    "    The Unknown text and End of Text Tokens are working properly.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3824f2df-4f3f-49a2-be1d-2add11ecc513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like some tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de58e8f-dadc-4170-8238-a42a71f1e5dd",
   "metadata": {},
   "source": [
    "**Here, The Hello and Palace are two unknown in the training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eeb60b-4a76-4674-9842-7bafd7bfc07b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
