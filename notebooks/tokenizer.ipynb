{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db94890e-cf7b-4ccb-9a96-fcfa90f41737",
   "metadata": {},
   "source": [
    "# Reading a short story as text sample in python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a571b4-d04a-4c40-aba7-c1021006bbd5",
   "metadata": {},
   "source": [
    "## Step 1: Creating tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227bc34d-597c-4b9f-b9a5-5e535298fddd",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: green; padding: 10px; border-radius: 5px;\">\n",
    "  The print command prints the total number of characters followed by the first 100 characters of this file for illustrations.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf8c8a6b-0fc1-4e1f-8678-7f98a1a05990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters:  20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"datasets/theVerdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Total number of characters: \", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7104fd4f-45ed-4e6a-803f-d592f800bbf7",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: green; padding: 10px; border-radius: 5px;\">\n",
    "    Our goal is to tokenize the 20479 characters short story into individual words and special characters that we can then turn into vector embeddings for LLM training.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71805090-83cb-4110-afdd-0d6564a9628c",
   "metadata": {},
   "source": [
    "**Regular Expression Library is used to split the sentences into individual texts.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c374715-6c06-4d89-ae33-7f8edcb49b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello, world. This, is a test\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60d59f1-db9e-4a10-b71d-1c9127bcc9f4",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: green; padding: 10px; border-radius: 5px;\">\n",
    "    The /s is regular expression for the whitespace so this will split the sentence according to white-spaces found.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491703d0-9ff6-4200-908f-6916217e2cce",
   "metadata": {},
   "source": [
    "**We can modify to split the whitespaces, periods and commas also i.e [,.]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87c40d06-41bd-4e3c-9a9e-f33133d97811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc62f105-da75-4aff-8db2-937614e5e4d7",
   "metadata": {},
   "source": [
    "**Removing The WhiteSpaces Safely**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb2478ec-36c4-4ffb-8ac1-7e624670b0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e28760e-cc4c-4e86-a991-d2d31f9f5644",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: green; padding: 10px; border-radius: 5px;\">\n",
    "    Firstly, for item in result is used to iterate through every items in the list, this will only return true if there is no white-spaces found, hence it will return the item and be stored in a newList allocated to result.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813d55b8-0854-4aeb-8565-a22c9b4f200c",
   "metadata": {},
   "source": [
    "**Modifying The Tokenizer Further To Accept quotations, question marks, double-dashes as a separate special characters or tokens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "154c600c-5a4e-4d70-8df7-917ca9d7ec37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18654fae-fd3c-4023-8fbf-765466d7ead5",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: green; padding: 10px; border-radius: 5px;\">\n",
    "    We use double item.strip() cause when splitting the special characters it might contain some whitespaces again so double checking ensures that no whitespacing is included in the final text.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f589f3b-adc6-4654-8d03-9b35d047a9f0",
   "metadata": {},
   "source": [
    "**Applying the tokenizer scheme to entire raw text data of the book**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "035d70c9-310d-4f59-a113-073f51757ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preProcessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preProcessed = [item.strip() for item in preProcessed if item.strip()]\n",
    "print(preProcessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44a95fce-78df-46eb-b9e4-dec6d0202749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "print(len(preProcessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304f175a-1489-4aca-ad95-4e29b0a829be",
   "metadata": {},
   "source": [
    "# Step 2: Assigning Token Id's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0fd221-9984-48d4-892f-3f9833565f4b",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: green; padding: 10px; border-radius: 5px;\">\n",
    "    Now, we take all the unique tokens together and sort them alphabetically\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7063407a-031a-4000-a27f-6077d3840584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preProcessed))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8125c1-8233-4d5f-b8b8-016fc1922a84",
   "metadata": {},
   "source": [
    "**Creating our own vocabulary with all the unique words by assigning integer values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bca50409-5f45-4715-98ce-554530b85012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token: integer for integer, token in enumerate(all_words)}\n",
    "for i,item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3d1198-91d9-4ad2-b56c-3dc9764e85a9",
   "metadata": {},
   "source": [
    "**Now Creating a class in python for encoding and decoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ae33465a-3222-4127-9b1e-85289758733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleTokenizerV1:\n",
    "    def __init__ (self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode (self, text):\n",
    "        preprocessed = re.split(r'([.,:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decoder (self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Remove the whitespaces before any specific characters:\n",
    "        text = re.sub(r'\\s+([,.:_?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a083c4-fbfe-40d1-be51-9352bdb02669",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: green; padding: 10px; border-radius: 5px;\">\n",
    "    Now, we instantiate the tokenizer class and try to pass the data. Encode Method converts the string into integers or token id's according to our vocabulary similarly, decode takes the list of ids and converts it back to the string.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b43ae742-1e29-4837-bc20-a80a8a2df4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 63, 2, 850, 731, 406, 584, 1016, 806, 203, 579, 265, 5, 1, 1077, 729, 722, 988, 189, 533, 598, 362, 127, 988, 87, 157, 890, 722, 156, 411, 168, 651, 5, 1090, 5, 727, 115, 604, 315, 5, 53, 514, 140, 849, 741, 477, 64, 24, 9, 157, 67, 7, 38, 5, 199, 727, 546, 5, 130, 456, 697, 391, 8, 1, 57, 584, 908, 684, 868, 1016, 403, 464, 722, 203, 7, 1]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = simpleTokenizerV1(vocab)\n",
    "text = \"\\\"Money's only excuse is to put beauty into circulation,\\\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed luncheon-table, when, on a later day, I had again run over from Monte Carlo; and Mrs. Gisburn, beaming on him, added for my enlightenment: \\\"Jack is so morbidly sensitive to every form of beauty.\\\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32a6b2e4-a9f1-479a-ba25-a0a9b72c9d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" Money' s only excuse is to put beauty into circulation,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed luncheon-table, when, on a later day, I had again run over from Monte Carlo ; and Mrs. Gisburn, beaming on him, added for my enlightenment:\" Jack is so morbidly sensitive to every form of beauty.\"\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decoder(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a609e7f1-03e9-4e7a-b43c-3889dedf8810",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow; color: red; padding: 10px; border-radius: 5px;\">\n",
    "   What if we send a sentence that doesn't belong to the training set or vocabulary?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26ee43c4-e162-484e-b051-280ca0d67f57",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, do you like some tea?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(ids)\n",
      "Cell \u001b[1;32mIn[22], line 9\u001b[0m, in \u001b[0;36msimpleTokenizerV1.encode\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m      7\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([.,:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m      8\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[1;32m----> 9\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "Cell \u001b[1;32mIn[22], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      7\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([.,:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m      8\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[1;32m----> 9\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Hello'"
     ]
    }
   ],
   "source": [
    "text = \"Hello, do you like some tea?\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24069678-e67e-4383-8090-94aed2df38b2",
   "metadata": {},
   "source": [
    "## Adding Special Context Tokens To Deal with data that is beyond the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9934ee-bef5-4090-b4e4-2747a178abd8",
   "metadata": {},
   "source": [
    "**We add two new tokens: <|unk|> which is used for token not part of the vocab and <|endoftext|> which was used for GPT pre-training to specify that the new text source is being used.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad0d7aef-ec03-499f-b33a-3ed2a2528706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preProcessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token: id for id, token in enumerate(all_tokens)}\n",
    "print(len(vocab.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4428c6c6-566b-4545-9521-ceefb0887543",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: green; padding: 10px; border-radius: 5px;\">\n",
    "    This increased the size of the vocabulary to 1132 previously it was 1130\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48278ba8-8155-4060-8207-7e7732372881",
   "metadata": {},
   "source": [
    "**Quickly printing the last 5 words in the vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42a9f631-2424-4d64-beda-d34756c6b98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5224954e-28ee-43d8-a2d8-dd545bf99da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleTokenizerV2:\n",
    "    def __init__ (self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode (self, text):\n",
    "        preprocessed = re.split(r'([.,:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_int\n",
    "                       else \"<|unk|>\" for item in preprocessed\n",
    "                       ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode (self, ids):\n",
    "        text = (\" \").join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.:_?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6673b080-1a23-4fa3-8ce3-9c1b51fc560b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 5, 355, 1126, 628, 910, 975, 10]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = simpleTokenizerV2(vocab)\n",
    "text = \"Hello, do you like some tea?\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8006dc28-9f63-4a10-a5bf-b026e9399c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like some tea?<|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like some tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \"<|endoftext|> \".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6a6cf0f3-f4d2-479a-8b63-0464d87a20a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131,\n",
       " 5,\n",
       " 355,\n",
       " 1126,\n",
       " 628,\n",
       " 910,\n",
       " 975,\n",
       " 10,\n",
       " 1130,\n",
       " 55,\n",
       " 988,\n",
       " 956,\n",
       " 984,\n",
       " 722,\n",
       " 988,\n",
       " 1131,\n",
       " 7]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618c333b-126e-40b0-bf07-6655920d72ff",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: green; padding: 10px; border-radius: 5px;\">\n",
    "    The Unknown text and End of Text Tokens are working properly.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3824f2df-4f3f-49a2-be1d-2add11ecc513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like some tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de58e8f-dadc-4170-8238-a42a71f1e5dd",
   "metadata": {},
   "source": [
    "**Here, The Hello and Palace are two unknown in the training dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca87642c-80ad-4c76-84d6-d0a76bf0214d",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding [Sub-Word Tokenizer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379ea981-fb68-4a59-9264-2237b1e2725a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow; color: red; padding: 10px; border-radius: 5px;\">\n",
    "  Since BPE is a complex algorithm so we will utilize the existing library in python developed by OpenAI used for the GPT model called tiktoken\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82399ceb-e3bc-40da-8301-95b596ee3b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\shubh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\shubh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\shubh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shubh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shubh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shubh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shubh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip3 install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cc5d0b-abc5-4572-afd5-f542dde3dcef",
   "metadata": {},
   "source": [
    "**Checking The Latest Version of TikToken**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "322adf0e-2cbc-4299-a8b1-fa0596fa92bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiktoken Version:  0.9.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "print(\"Tiktoken Version: \", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c171808-250a-428c-afd2-42b87bd94171",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: green; padding: 10px; border-radius: 5px;\">\n",
    "    Once installed we can instantiate the BPE tokenizer from tiktoken as follows:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d25da2b-78b0-46de-aa66-92f0bc56f589",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc28de80-516d-4b64-a353-ecb57f2f383b",
   "metadata": {},
   "source": [
    "**Now, Testing the methods, just like: simpleTokenizerV2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b3afb4c-7bc8-4a17-afb9-bf23cac8d027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5211, 345, 588, 617, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\"Do you like some tea? <|endoftext|> In the sunlit terraces\"\"of someunknownPlace.\")\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c0419cd-3e35-4625-a9d9-7af48c047586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you like some tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73068a8d-5138-46dc-9c8f-9ef3f6c9228d",
   "metadata": {},
   "source": [
    "**Another example of simple encoding and decoding done by BPE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9ecb326-6eba-4f1b-8efb-b4d4324aa8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n",
      "Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "integers = tokenizer.encode(\"Akwirw ier\")\n",
    "print(integers)\n",
    "text = tokenizer.decode(integers)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341d4207-72b2-4a4f-8380-d00a81d7ed51",
   "metadata": {},
   "source": [
    "# Input Target Pairs In Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f07e987-6960-4101-80e9-2ddd17ab1a2c",
   "metadata": {},
   "source": [
    "<div style = \"color:green; background-color:lightgreen; padding: 10px; border-radius: 5px\">\n",
    "    In this lecture, I will learn to implement data loader that will fetch the input-target pairs using sliding window approach.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed7e0fb-032f-478e-8fc4-192cfce46e31",
   "metadata": {},
   "source": [
    "**We will firstly encode the entire dataset using the BPE tokenizer from tiktoken**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67bb5830-7d2b-4e30-b21c-e2dd514275cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\shubh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\shubh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\shubh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shubh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shubh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shubh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shubh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
      "Tiktoken Version:  0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tiktoken\n",
    "\n",
    "import importlib\n",
    "import tiktoken\n",
    "import re\n",
    "\n",
    "# Current Version of tiktoken:\n",
    "print(\"Tiktoken Version: \", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcb62f1-ceb6-46ba-a543-0586a5352fbc",
   "metadata": {},
   "source": [
    "**Instantiate the BPE tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ce00a5d-cfcf-4993-ba67-e5fbe083ceb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15496, 11, 2159, 0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "text = \"Hello, World!\"\n",
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3eb58d-4764-4c6f-8b6b-77b35c3c0dbc",
   "metadata": {},
   "source": [
    "**Passing The Dataset To The BPE Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2492f964-1623-4f99-8986-418f5079ba72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5110\n"
     ]
    }
   ],
   "source": [
    "with open('datasets/theVerdict.txt', 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "clean_text = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "clean_text = [item.strip() for item in clean_text if item.strip()]\n",
    "enc_text = tokenizer.encode(\" \".join(clean_text), allowed_special={\"<|endoftext|>\"})\n",
    "print(len(enc_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4272f2b0-1169-4c10-880d-a30239297ffd",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow; color: red; padding: 10px; border-radius: 5px;\">\n",
    "  The total size of the vocab utilized for this entire data-set will be around: 5,145\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4157fe42-986c-4125-a532-ea2adde1edb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aa041b-f713-4a8f-8ecc-b4d88c74766d",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: green; padding: 10px; border-radius: 5px;\">\n",
    "  The context size is the number of input tokens that will be provided to the LLM's to predict the next target pair.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52b34e73-f770-436f-9457-bf2ebde0fa5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4 # Total input that the model will access\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed79f1e-bce5-47e1-94df-5b746af1d868",
   "metadata": {},
   "source": [
    "**The inputs shifted by one position, we can then create the next-word prediction task as follows:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60873fe1-fb70-403e-b09e-650e0d1e3c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] -------> 4920\n",
      "[290, 4920] -------> 2241\n",
      "[290, 4920, 2241] -------> 287\n",
      "[290, 4920, 2241, 287] -------> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range (1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, '------->',  desired)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a881114-e562-47b1-bbc0-a8c4922dde2e",
   "metadata": {},
   "source": [
    "**Now, converting the ids or tokens into the respective texts to clear the understanding.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ccadc9e-52b3-4595-a628-d3ee95a58e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and -------->  established\n",
      " and established -------->  himself\n",
      " and established himself -------->  in\n",
      " and established himself in -------->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range (1, context_size + 1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), '-------->', tokenizer.decode([desired]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
